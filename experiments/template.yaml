#Experiment configuration file.

#PT Lightning trainer arguments.
#Documentation: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
#Comment out options you aren't using
train:
  accelerator: #str, e.g. "gpu"
  devices: #int
  num_nodes: #int
  strategy: #str
  precision: #str
  auto_scale_batch_size: #bool
  auto_select_gpus: #bool
  enable_checkpointing: #bool
  check_val_every_n_epoch: #int
  default_root_dir: #str
  logger: #bool
  log_every_n_steps: #int
  enable_progress_bar: #bool
  profiler: #str
  max_epochs: #int
  max_steps: #int

#Model arguments
model:
  type: #str <CAE|QCAE|QCNN>
  module: #str <skip|pool>, CAE and QCAE only
  spatial_dim: #int, AE only
  mlp_dim: #int, QCNN only
  latent_dim: #int, AE only
  point_seq: #list[int], QC only
  stages: #int
  loss_fn: #str, SobolevLoss or loss from torch.nn, AE only
  optimizer: #str, optimizer from torch.optim
  learning_rate: #float

  conv_params:
    in_points: #list[int]
    out_points: #list[int]
    in_channels: #list[int]
    out_channels: #list[int]
    bias: #list[bool]
    filter_seq: #[list[int]], QC only
    filter_mode: #[string] <single|share_in|nested>, QC only

#PT LightningDataModule arguments
data:
  module: #str <grid_data|mesh_data|modelnet_data>
  data_dir: #str
  spatial_dim: #int
  num_points: #int, mesh only
  batch_size: #int
  size: #int, grid only
  stride: #int, grid only
  channels: #list[int], AE only
  shuffle: #bool

#Miscellaneous arguments
misc:
  make_gif: #bool, AE only
  compute_stats: #bool
  early_stopping: #bool
