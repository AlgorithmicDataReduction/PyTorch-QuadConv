#Experiment configuration file.

#PT Lightning trainer arguments.
#Documentation: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
#Comment out options you aren't using
train:
  accelerator: "gpu" #str e.g. "gpu"
  devices: -1 #int
  # num_nodes: #int
  # strategy: #str
  # precision: #str
  # auto_scale_batch_size: #bool
  # auto_select_gpus: #bool
  enable_checkpointing: True #bool
  check_val_every_n_epoch: 1 #int
  # default_root_dir: #str
  logger: True #bool
  # log_every_n_steps: #int
  enable_progress_bar: True #bool
  # profiler: #str
  max_epochs: 3000 #int
  # max_steps: #int

#Model arguments
model:
  conv_type: "standard" #str "standard" or "quadrature"
  point_dim: 1 #int
  latent_dim: 10 #int
  point_seq: [100, 75, 50, 25] #list
  channel_seq: [1, 4, 16, 32, 32] #list
  # mlp_channels: #list, delete this for standard conv
  optimizer: "adam" #str
  learning_rate: 0.01 #float

#PT LightningDataModule arguments
data:
  data_dir: "data/simple_transport" #str
  dimension: 1 #int
  batch_size: 10 #int
  size: 100 #int
  stride: 100 #int
  flatten: False #bool, flatten for quadrature
  channels: [0] #list
  shuffle: False #bool

#Extra arguments
extra:
  make_gif: True #bool
  early_stopping: False #bool
